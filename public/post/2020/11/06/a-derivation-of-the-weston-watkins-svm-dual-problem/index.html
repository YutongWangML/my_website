<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>The Weston-Watkins SVM dual problem | Yutong Wang</title>
    <link rel="stylesheet" href="/~yutongw/css/style.css" />
    <link rel="stylesheet" href="/~yutongw/css/fonts.css" />
    
  </head>

  <body>
    <nav>
    <ul class="menu">
      
      <li><a href="/~yutongw">Home</a></li>
      
      <li><a href="/~yutongw/publications/">Publications</a></li>
      
      <li><a href="/~yutongw/post/">Blog</a></li>
      
      <li><a href="/~yutongw/cv.pdf">CV</a></li>
      
    </ul>
    <hr/>
    </nav>

<div class="article-meta">
<h1><span class="title">The Weston-Watkins SVM dual problem</span></h1>
<h2 class="author">Yutong Wang</h2>
<h2 class="date">2020/11/06</h2>
</div>

<main>



<p>In this blog post, we derive the dual problem for the Weston-Watkins support vector machine (SVM).
The dual optimization for the Weston-Watkins SVM can be found in the literature, e.g.,
<span class="citation">Keerthi et al. (2008)</span>
and
<span class="citation">Weston and Watkins (1999)</span>.
However, they often omit the tedious derivation.</p>
<div id="set-up-and-notations" class="section level1">
<h1>Set-up and notations</h1>
<p>Let <span class="math inline">\(k \ge 2\)</span> be an integer representing the number of classes and <span class="math inline">\([k] = \{1,\dots, k\}\)</span>.</p>
<p>Let <span class="math inline">\(n\)</span> be the size of the training data.</p>
<p>For each <span class="math inline">\(i \in [n]\)</span>, <span class="math inline">\(x_i \in \mathbb{R}^d\)</span> is column vector and <span class="math inline">\(y_i \in [k]\)</span>.</p>
<p>Let <span class="math inline">\(W = [w_1,\dots, w_k] \in \mathbb{R}^{d\times k}\)</span> where <span class="math inline">\(w_j\)</span> is the <span class="math inline">\(j\)</span>-th column of <span class="math inline">\(W\)</span>.</p>
<p>Let <span class="math inline">\(e_i \in \mathbb{R}^k\)</span> be the <span class="math inline">\(i\)</span>-th elementary basis (column) vector.</p>
</div>
<div id="primal-problem" class="section level1">
<h1>Primal problem</h1>
<p>The Weston-Watkins SVM minimizes over <span class="math inline">\(W\)</span> the following regularized empirical risk:</p>
<p><span class="math display">\[
\frac{1}{2}\|W\|_F^2 + C \sum_{i=1}^n \sum_{j \in [k]: j \ne y_i} \max \{0, 1- (w_{y_i}&#39; x_i - w_j&#39;x_i)\}.
\]</span></p>
<p>If <span class="math inline">\(\widetilde W = [\widetilde w_1,\dots, \widetilde w_k]\)</span> is the optimizer, then the classifier is
<span class="math display">\[
x 
\mapsto 
\mathrm{argmax}_{j \in [k]}
\widetilde w_j &#39;x.
\]</span></p>
<p>Now, for each <span class="math inline">\(j, l \in [k]\)</span>, let <span class="math inline">\(\Delta_{j,l} = e_j - e_l\)</span>.
Then we can rewrite the regularized empirical risk as</p>
<p><span class="math display">\[
\frac{1}{2}\|W\|_F^2 + C \sum_{i=1}^n \sum_{j \in [k]: j \ne y_i} \max \{0, 1- \Delta_{y_i, j}&#39; W&#39; x_i\}
\]</span></p>
<p>Introducing the slack variable <span class="math inline">\(\xi_{ij}\)</span>, we can minimize</p>
<p><span class="math display">\[
\frac{1}{2}\|W\|_F^2 + C \sum_{i=1}^n \sum_{j \in [k]: j \ne y_i} \xi_{ij}
\]</span></p>
<p>subject to</p>
<p><span class="math display">\[
\begin{cases}
 \xi_{ij} \ge 0 \\
\xi_{ij} \ge 1 - \Delta_{y_i, j}&#39; W&#39; x_i
\end{cases}
\]</span></p>
<p>or, equivalently, subject to</p>
<p><span class="math display">\[
\begin{cases}
0 \ge 1 - \mathrm{tr}( W&#39; x_i\Delta_{y_i, j}&#39;) - \xi_{ij}\\
  0 \ge -\xi_{ij} 
\end{cases}
\]</span></p>
<p>where <span class="math inline">\(\mathrm{tr}\)</span> is the trace operator. We observe that <span class="math inline">\(W&#39; x_i\Delta_{y_i, j}&#39; \in \mathbb{R}^{k\times k}\)</span>.</p>
</div>
<div id="dual-problem" class="section level1">
<h1>Dual problem</h1>
<p>Let <span class="math inline">\(\alpha_{ij} \ge 0\)</span> and <span class="math inline">\(\beta_{ij} \ge 0\)</span> be the dual variables for the above constraints, respectively.</p>
<p>The Lagrangian is</p>
<p><span class="math display">\[
L(W, \xi, \alpha, \beta)
=
\frac{1}{2}\|W\|_F^2 
+ \sum_{i=1}^n \sum_{j \in [k]: j \ne y_i} C \xi_{ij}
-
\beta_{ij}\xi_{ij}
+
\alpha_{ij}
(1 - \mathrm{tr}( W&#39; x_i\Delta_{y_i, j}&#39;) - \xi_{ij})
\]</span></p>
<p>Rearranging, we get</p>
<p><span class="math display">\[
L(W, \xi, \alpha, \beta)
=
\frac{1}{2}\|W\|_F^2 
+ \sum_{i=1}^n \sum_{j \in [k]: j \ne y_i} 
   \xi_{ij}
  (C
-
\beta_{ij}
-\alpha_{ij})
+
\alpha_{ij}
(1 - \mathrm{tr}( W&#39; x_i\Delta_{y_i, j}&#39;))
\]</span></p>
<p>Setting to zero the gradient of <span class="math inline">\(L\)</span> with respect to <span class="math inline">\(W\)</span>, we get</p>
<p><span class="math display">\[
0 = \nabla_W L
=
W
- 
  \sum_{i=1}^n \sum_{j \in [k]: j \ne y_i} 
\alpha_{ij}
 x_i\Delta_{y_i, j}&#39;
\]</span></p>
<p>Equivalently,
<span class="math display">\[
W
= 
  \sum_{i=1}^n \sum_{j \in [k]: j \ne y_i} 
\alpha_{ij}
 x_i\Delta_{y_i, j}&#39;
\]</span></p>
<p>Next, setting to zero the gradient of <span class="math inline">\(L\)</span> with respect to <span class="math inline">\(\xi_{ij}\)</span>, we get</p>
<p><span class="math display">\[
0 = \nabla_{\xi_{ij}} L
=
C
-
\beta_{ij} - \alpha_{ij}.
\]</span></p>
<p>Thus, the dependencies in the Lagrangian on <span class="math inline">\(\xi_{ij}\)</span> and <span class="math inline">\(\beta_{ij}\)</span> are removed and so</p>
<p><span class="math display">\[
L(W, \alpha)
=
\frac{1}{2}\|W\|_F^2 
+ \sum_{i=1}^n \sum_{j \in [k]: j \ne y_i} 
\alpha_{ij}
(1 - \mathrm{tr}( W&#39; x_i\Delta_{y_i, j}&#39;))
\]</span></p>
<p>Now,
<span class="math display">\[
\sum_{i=1}^n \sum_{j \in [k]: j \ne y_i} 
\alpha_{ij}
\mathrm{tr}( W&#39; x_i\Delta_{y_i, j}&#39;)
=
\mathrm{tr}\left(
W&#39;
\sum_{i=1}^n \sum_{j \in [k]: j \ne y_i} 
\alpha_{ij}
 x_i\Delta_{y_i, j}&#39;\right)
=
\mathrm{tr}(W&#39;W)
=
\|W\|_F^2.
\]</span></p>
<p>Hence, we have</p>
<p><span class="math display">\[
L(W, \alpha)
=
-\frac{1}{2}\|W\|_F^2 
+ \sum_{i=1}^n \sum_{j \in [k]: j \ne y_i} 
\alpha_{ij}
\]</span></p>
<p>Now, let <span class="math inline">\(\alpha_{iy_i} = -\sum_{j\in[n]: j \ne y_i} \alpha_i\)</span>. Using the definition of <span class="math inline">\(\Delta_{y_i, j} = e_{y_i} - e_j\)</span>, we get
<span class="math display">\[
W
= 
  \sum_{i=1}^n \sum_{j \in [k]: j \ne y_i} 
\alpha_{ij}
 x_i\Delta_{y_i, j}&#39;
 =
  \sum_{i=1}^n 
  \sum_{j \in [k]: j \ne y_i} 
  -
\alpha_{ij}
 x_i
 e_j&#39;
 +
 \sum_{i=1}^n
 (
  \sum_{j \in [k]: j \ne y_i} 
  \alpha_{ij}
  )
 x_i
 e_{y_i}&#39;
 =
 -
 \sum_{i=1}^n
 \sum_{j \in [k]}
 \alpha_{ij}
 x_i
 e_j&#39;
\]</span></p>
<p>Now, let us define the column vector
<span class="math display">\[
\alpha_i
=
\begin{bmatrix}
\alpha_{i1}\\
\vdots \\
\alpha_{ik}
\end{bmatrix}
\in 
\mathbb{R}^k
\]</span>
Then we have
<span class="math display">\[
W = 
-
\sum_{i = 1}^n 
x_i \alpha_i&#39;
\]</span>
where we observe that <span class="math inline">\(x_i \alpha_i&#39; \in \mathbb{R}^{d \times k}\)</span>.</p>
<p>Now,
<span class="math display">\[
\|W\|_F^2
=
\mathrm{tr}
(W&#39;W)
=
\mathrm{tr}
\left(
\sum_{i, s\in [n]}
\alpha_s x_s&#39; x_i \alpha_i&#39;
\right)
=
\mathrm{tr}
\left(
\sum_{i, s\in [n]}
 x_s&#39; x_i \alpha_i&#39;
\alpha_s
\right)
=
\sum_{i, s\in [n]}
 x_s&#39; x_i \alpha_i&#39;
\alpha_s.
\]</span>
This eliminates the dependencies in the Lagrangian on <span class="math inline">\(W\)</span> and so
<span class="math display">\[
L(\alpha)
=
-\frac{1}{2} 
\sum_{i, s\in [n]}
 x_s&#39; x_i \alpha_i&#39;
\alpha_s
+
\sum_{i \in [n]} 
\sum_{j \in [k]: j \ne y_i}
\alpha_{ij}
\]</span></p>
<p>Thus, the dual problem is</p>
<p><span class="math display">\[
\mathrm{minimize}_{\alpha}
\quad
f(\alpha) :=
\frac{1}{2} 
\sum_{i, s\in [n]}
 x_s&#39; x_i \alpha_i&#39;
\alpha_s
-
\sum_{i \in [n]} 
\sum_{j \in [k]: j \ne y_i}
\alpha_{ij}
\]</span>
subject to
<span class="math display">\[
\begin{cases}
0 \le \alpha_{ij} \le C &amp;: j \ne y_i\\
\alpha_{iy_i}
=
\sum_{j \in [k]: j \ne y_i} - \alpha_j &amp;: j = y_i.
\end{cases}
\]</span></p>
<p>This is the same as equation (16) of <span class="citation">Keerthi et al. (2008)</span>. Actually, there is a typo in (16) where the <span class="math inline">\(\alpha_{ij}\)</span>s should have a negative sign in front.
Later, <span class="citation">Keerthi et al. (2008)</span> computes the derivative of <span class="math inline">\(f\)</span> with the correct sign.</p>
</div>
<div id="derivatives" class="section level1">
<h1>Derivatives</h1>
<p>Below, fix such a <span class="math inline">\(i \in [n]\)</span> and <span class="math inline">\(j \in [k]\)</span> such that <span class="math inline">\(j \ne y_i\)</span>.</p>
<p>In this section, we compute <span class="math inline">\(\frac{\partial f}{\partial \alpha_{ij}}\)</span>.</p>
<p>First, let <span class="math inline">\(i \ne s\)</span> and consider the term
<span class="math display">\[
x_s&#39; x_i \alpha_i&#39;\alpha_s
=
x_s&#39; x_i (\alpha_{ij} \alpha_{sj}
+
\alpha_{iy_i} \alpha_{sy_i}
+
\mathrm{constant}
)
\]</span>
where <span class="math inline">\(\mathrm{constant}\)</span> are terms that do not depend on <span class="math inline">\(\alpha_{ij}\)</span>.</p>
<p>Taking derivative of <span class="math inline">\(x_s&#39; x_i \alpha_i&#39;\alpha_s\)</span> with respect to <span class="math inline">\(\alpha_{ij}\)</span>, we get</p>
<p><span class="math display">\[
x_s&#39; x_i (\alpha_{sj}
- \alpha_{sy_i})
\]</span>
where we recall that <span class="math inline">\(\alpha_{iy_i} = \sum_{l \in [k]: l \ne y_i} -\alpha_{il}\)</span>.</p>
<p>Similarly, when <span class="math inline">\(i = s\)</span>, we have that the derivative of <span class="math inline">\(x_i&#39; x_i \alpha_i&#39;\alpha_i\)</span> is</p>
<p><span class="math display">\[
2 x_i&#39; x_i (\alpha_{ij} - \alpha_{i y_i|}).
\]</span></p>
<p>From this, we get that
<span class="math display">\[
\frac{\partial f}{\partial \alpha_{ij}} (\alpha)
=
-1
+
\sum_{s \in [n]} x_i&#39; x_s (\alpha_{sj} - \alpha_{s y_i})
\]</span></p>
<p>Now, observe that</p>
<p><span class="math display">\[
\sum_{s \in [n]} x_i&#39; x_s (\alpha_{sj} - \alpha_{s y_i})
=
x_i &#39;
\left(\sum_{s \in [n]} \alpha_{sj} x_s  -\alpha_{s y_i} x_s\right)
\]</span></p>
<p>Recall from earlier that</p>
<p><span class="math display">\[
W = 
-
\sum_{i = 1}^n 
x_i \alpha_i&#39;
\]</span></p>
<p>Thus,
<span class="math display">\[
w_j
=
We_j = 
-
\sum_{i = 1}^n 
\alpha_{ij}x_i
\quad \mbox{and} \quad
w_{y_i}=
We_{y_i} = 
-
\sum_{i = 1}^n 
\alpha_{iy_i}x_{i}.
\]</span></p>
<p>Thus, we get
<span class="math display">\[
x_i &#39;
\left(\sum_{s \in [n]} \alpha_{sj} x_s  -\alpha_{s y_i} x_s\right)
=
w_{y_i}&#39; x_i 
- w_{j} &#39; x_i.
\]</span></p>
<p>From this, we conclude that
<span class="math display">\[
\frac{\partial f}{\partial \alpha_{ij}} (\alpha)
=
w_{y_i}&#39; x_i - w_{j}&#39;x_i
-1.
\]</span>
Since we are minimizing, it is more convenient to consider the negative gradient:
<span class="math display">\[
-\frac{\partial f}{\partial \alpha_{ij}} (\alpha)
=
1-
(w_{y_i}&#39; x_i - w_{j}&#39;x_i).
\]</span>
Note that this equivalent to equation (17) from <span class="citation">Keerthi et al. (2008)</span>.</p>
</div>
<div id="coordinate-descent" class="section level1">
<h1>Coordinate descent</h1>
<p>In this section, we again fix <span class="math inline">\(i \in [n]\)</span> and <span class="math inline">\(j \in [k]\)</span> such that <span class="math inline">\(j \ne y_i\)</span>.
We will explain how <a href="https://github.com/Shark-ML/Shark/blob/7a182c7923e94cf6a8d65b6c92a162bafad8314c/include/shark/Algorithms/QP/QpMcLinear.h#L387">Shark</a> (<span class="citation">Igel, Heidrich-Meisner, and Glasmachers (2008)</span>) solves the subproblem:</p>
<p><span class="math display">\[
\mathrm{minimize}_{\alpha_{i1},\dots, \alpha_{ik}}
\quad
f(\alpha) :=
\frac{1}{2} 
\sum_{i, s\in [n]}
 x_s&#39; x_i \alpha_i&#39;
\alpha_s
-
\sum_{i \in [n]} 
\sum_{j \in [k]: j \ne y_i}
\alpha_{ij}
\]</span>
subject to
<span class="math display">\[
\begin{cases}
0 \le \alpha_{ij} \le C &amp;: j \ne y_i\\
\alpha_{iy_i}
=
\sum_{j \in [k]: j \ne y_i} - \alpha_j &amp;: j = y_i.
\end{cases}
\]</span></p>
<p>Note that <span class="math inline">\(\alpha_{st}\)</span> is fixed for all <span class="math inline">\(s \ne i\)</span> and all <span class="math inline">\(j \in [k]\)</span>. By cycling through the <span class="math inline">\(i\in [n]\)</span> and solving the above subproblem, we get a form of coordinate descent.</p>
<p>We consider the updates
<span class="math display">\[
\begin{cases}
\hat{\alpha}_{st} = \alpha_{st} &amp;: s \ne i \mbox{ or } t \ne j \\
\hat{\alpha}_{ij} = \alpha_{ij} + m &amp; \mbox{otherwise.}
\end{cases}
\]</span></p>
<p>Similarly, define</p>
<p><span class="math display">\[
\widehat W = [\hat w_1, \cdots \hat w_k]
=
\sum_{i = 1}^n 
x_i \hat \alpha_i&#39;.
\]</span></p>
<p>Then by construction, we have for <span class="math inline">\(t \in [k]\)</span> that</p>
<p><span class="math display">\[
\hat w_t
=
\begin{cases}
w_t &amp;: t \not \in \{j, y_i\}\\
w_j - mx_i &amp;: t = j \\
w_{y_i} + mx_i &amp;: t = y_i.
\end{cases}
\]</span></p>
<p>Thus, we have
<span class="math display">\[
-\frac{\partial f}{\partial \alpha_{it}}( \hat \alpha)
= 
1 - (\hat w_{y_i} &#39; x_i - \hat w_t&#39; x_i)
=
\begin{cases}
1 - (w_{y_i} &#39; x_i - w_j&#39; x_i) - 2 m \|x_i\|^2 &amp;: t = j \\
1 - (w_{y_i} &#39; x_i - w_t&#39; x_i) - 1 m \|x_i\|^2 &amp;: t \ne j \\
\end{cases}
\]</span></p>
<p>In other words, we have</p>
<p><span class="math display">\[
-\frac{\partial f}{\partial \alpha_{it}}( \hat \alpha)
=
\begin{cases}
-\frac{\partial f}{\partial \alpha_{ij}}( \alpha) - 2 m \|x_i\|^2 &amp;: t = j \\
-\frac{\partial f}{\partial \alpha_{it}}( \alpha) - 1 m \|x_i\|^2 &amp;: t \ne j. \\
\end{cases}
\]</span>
This is the mathematical justification behind this <a href="https://github.com/Shark-ML/Shark/blob/7a182c7923e94cf6a8d65b6c92a162bafad8314c/include/shark/Algorithms/QP/QpMcLinear.h#L480">code snippet</a> from Shark.</p>
<hr />
<div id="refs" class="references">
<div id="ref-igel2008shark">
<p>Igel, Christian, Verena Heidrich-Meisner, and Tobias Glasmachers. 2008. “Shark.” <em>Journal of Machine Learning Research</em> 9 (Jun): 993–96.</p>
</div>
<div id="ref-keerthi2008sequential">
<p>Keerthi, S Sathiya, Sellamanickam Sundararajan, Kai-Wei Chang, Cho-Jui Hsieh, and Chih-Jen Lin. 2008. “A Sequential Dual Method for Large Scale Multi-Class Linear Svms.” In <em>Proceedings of the 14th Acm Sigkdd International Conference on Knowledge Discovery and Data Mining</em>, 408–16.</p>
</div>
<div id="ref-weston1999support">
<p>Weston, J, and C Watkins. 1999. “Support Vector Machines for Multi-Class Pattern Recognition.” In <em>Proc. 7th European Symposium on Artificial Neural Networks, 1999</em>.</p>
</div>
</div>
</div>

</main>

  <footer>
  <script src="//yihui.name/js/math-code.js"></script>
<script async src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<script async src="//yihui.name/js/center-img.js"></script>

  
  </footer>
  </body>
</html>


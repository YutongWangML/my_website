{ "class": "GraphLinksModel",
  "nodeDataArray": [ 
{"text":"earlier work has focused on least squares \n(or ridge) regression with square loss", "color":"white", "key":-3, "location":"-1086.6707336794987 656.6769749457915"},
{"location":"-1374.0661848264822 364.481876967457", "text":"montanari2019generalization\nMontanari, Andrea\nRuan, Feng\nSohn, Youngtak\nYan, Jun\n2019\nThe generalization error of max-margin linear classifiers: \nHigh-dimensional asymptotics in the overparametrized regime", "color":"lightyellow", "key":-6, "group":-54, "font":"bold 14pt serif"},
{"location":"-148.42572416426265 1235.3941198986058", "text":"bartlett2020benign\nBartlett, Peter L\nLong, Philip M\nLugosi, G{\\'a}bor\nTsigler, Alexander\n2020\nBenign overfitting in linear regression", "color":"lightblue", "key":-7, "group":-53, "scale":1},
{"location":"-636.8535038095058 1018.1035145937979", "text":"hastie2019surprises\nHastie, Trevor\nMontanari, Andrea\nRosset, Saharon\nTibshirani, Ryan J\n2019\nSurprises in high-dimensional \nridgeless least squares interpolation", "color":"lightyellow", "key":-8, "group":-53},
{"location":"-1161.661108867445 1228.6620509041175", "text":"liang2020just\nLiang, Tengyuan\nRakhlin, Alexander\n2020\nJust interpolate: \nKernel ``ridgeless'' regression can generalize", "color":"lightyellow", "key":-9, "group":-53},
{"location":"-255.30811425261868 164.05434940856674", "text":"deng2019model\nDeng, Zeyu\nKammoun, Abla\nThrampoulidis, Christos\n2019\nA model of double descent for \nhigh-dimensional binary linear classification", "color":"lightblue", "key":-10, "group":-54},
{"location":"-1629.414418447599 -1474.0056328668659", "text":"theisen2020good\nTheisen, Ryan\nKlusowski, Jason M\nMahoney, Michael W\n2020\nGood linear classifiers are \nabundant in the interpolating regime", "color":"lightyellow", "key":-11},
{"text":"investigates the classification performance of \nhard-margin SVM for binary linear classification \nunder generative data models", "color":"white", "key":-12, "location":"-763.4521208531662 228.69848960677552", "group":-54},
{"location":"-1418.940022184353 -622.7613966727171", "text":"chatterji2020finite\nChatterji, Niladri S\nLong, Philip M\n2020\nFinite-sample analysis of \ninterpolating linear classifiers \nin the overparameterized regime", "color":"lightyellow", "key":-14, "group":-54},
{"text":"“double descent” phenomena in", "color":"white", "key":-15, "location":"-1476.965442559109 -1073.1274155805272"},
{"location":"-1383.8731235166251 -110.5731493584218", "text":"liang2020precise\nLiang, Tengyuan\nSur, Pragya\n2020\nA precise high-dimensional \nasymptotic theory for boosting and \nmin-l1-norm interpolated classifiers", "color":"lightyellow", "key":-16, "group":-54},
{"location":"-2622.771652045425 -418.16490111034045", "text":"mei2019generalization\nMei, Song\nMontanari, Andrea\n2019\nThe generalization error of \nrandom features regression: \nPrecise asymptotics and \ndouble descent curve", "color":"lightblue", "key":-17, "group":-31},
{"text":"asymptotic error of the OLS estimator \nin the random features model", "color":"white", "key":-18, "location":"-2029.8308909462235 -564.6209635096174"},
{"location":"-2292.207419511561 -1600.326360538881", "text":"harzli2021double\nHarzli, Ouns El\nValle-P{\\'e}rez, Guillermo\nLouis, Ard A\n2021\nDouble-descent curves in \nneural networks: \na new perspective using \nGaussian processes", "color":"lightyellow", "key":-19},
{"location":"-2982.6745118172407 1632.6260105938115", "text":"zhang2016understanding\nZhang, Chiyuan\nBengio, Samy\nHardt, Moritz\nRecht, Benjamin\nVinyals, Oriol\n2016\nUnderstanding deep learning \nrequires rethinking generalization", "color":"lightyellow", "key":-20},
{"text":" largely motivated by the empirical observation that \ndeep neural networks fit perfectly the data and yet generalize well", "color":"white", "key":-21, "location":"-2789.858330217658 1361.237777523962"},
{"location":"-2462.7799163130167 1672.0668489190211", "text":"belkin2019reconciling\nBelkin, Mikhail\nHsu, Daniel\nMa, Siyuan\nMandal, Soumik\n2019\nReconciling modern machine-learning practice \nand the classical bias--variance trade-off", "color":"lightyellow", "key":-22},
{"text":"A common phenomenology has been demonstrated\nempirically in a number of statistical models, including kernel methods, random forests, and multilayer\nneural networks", "color":"white", "key":-23, "location":"-2401.9931338566334 1490.6730132355947"},
{"location":"-2692.7056484409795 -907.0608260768748", "text":"liao2020random\nLiao, Zhenyu\nCouillet, Romain\nMahoney, Michael W\n2020\nA random matrix analysis of \nrandom Fourier features: \nbeyond the Gaussian kernel, \na precise phase transition, and \nthe corresponding double descent", "color":"lightyellow", "key":-24, "group":-31},
{"text":"most relevant (random matrix theory) work...\n...improve their results by considering \ngeneric data model on the nonlinear RFF model\n...extends the analysis in [mei2019generalization] to handle the\n RFF model and its phase structure on real-world data sets", "color":"white", "key":-25, "location":"-2624.0988165697804 -633.9132639339247", "group":-31},
{"scale":1, "text":"[liao2020random] studies a double descent behavior in \nrandom Fourier feature regression. \nbut the analysis is not carried in the context of neural networks \ndo not consider the result of the full Bayesian training as we do.", "color":"white", "key":-26, "location":"-2380.3574144448066 -1390.1227553701162"},
{"location":"508.4367311341932 1104.6070724244025", "text":"belkin2020two\nBelkin, Mikhail\nHsu, Daniel\nXu, Ji\n2020\nTwo models of double descent for weak features", "color":"lightyellow", "key":-27, "group":-53},
{"text":"calculated the excess risk for certain linear models", "color":"white", "key":-29, "location":"249.99412713809807 1299.3506360741965", "group":-53},
{"text":"linear regression in an asymptotic regime", "color":"white", "key":-30, "location":"-640.3816270529283 1276.6363636112465", "group":-53},
{"text":"Random features", "isGroup":true, "color":"blue", "key":-31},
{"text":"while random features are highly non-Gaussian,\nit is possible to construct a Gaussian covariates model\nwith the same asymptotic prediction error\n...\nuses this Gaussian covariates proxy \nto analyze maximum margin classification \nusing random features", "color":"white", "key":-32, "location":"-2246.4502324053165 -59.75471440807394"},
{"location":"-589.2969536960939 -1524.8413321700905", "text":"liang2021interpolating\nLiang, Tengyuan\nRecht, Benjamin\n2021\nInterpolating Classifiers Make Few Mistakes", "color":"lightyellow", "key":-33},
{"text":"used generative models to understand how such margins scale", "color":"white", "key":-34, "location":"-885.2336053956042 -1018.6748079115713"},
{"location":"-53.9891633718039 -989.2197871408297", "text":"liang2020multiple\nLiang, Tengyuan\nRakhlin, Alexander\nZhai, Xiyu\n2020\nOn the multiple descent of \nminimum-norm interpolants and \nrestricted lower isometry of kernels", "color":"lightyellow", "key":-35},
{"text":"generalization of kernel ridgeless regression", "color":"white", "key":-36, "location":"-320.7090768874557 -1236.8787655389597"},
{"text":"[the double descent phenomenon] is essentially\n a consequence of a transition between \ntwo different phases of learning ", "color":"white", "key":-37, "location":"-1998.720192550893 -988.4176918948835"},
{"text":"considered minimum norm interpolating kernel regression with kernels defined\nas nonlinear functions of the Euclidean inner product and showed that, with certain properties of\nthe training sample (expressed in terms of the empirical kernel matrix), these methods can have\ngood prediction accuracy.", "color":"white", "key":-38, "location":"-574.3177908739103 1439.070790121735", "group":-53},
{"text":"marginal distribution over the covariates \nis a mixture of Gaussians, one for each class", "color":"white", "key":-39, "location":"-925.7015437743476 -348.2939620508053", "group":-54},
{"text":"asymptotic test error of the \nmaximum L1-margin classifier", "color":"white", "key":-41, "location":"-1376.8944436983797 -333.3262583711971", "group":-54},
{"text":"a formula for the asymptotic test error of\nthe maximum margin classifier in the \noverparameterized setting when \nthe features are generated from \na Gaussian distribution and \nthe labels are generated from \na logistic link function", "color":"white", "key":-42, "location":"-1067.5200414972542 -176.1943312042468", "group":-54},
{"location":"-233.194275263877 -304.87981578262674", "text":"muthukumar2020classification\nMuthukumar, Vidya\nNarang, Adhyyan\nSubramanian, Vignesh\nBelkin, Mikhail\nHsu, Daniel\nSahai, Anant\n2020\nClassification vs regression \nin overparameterized regimes: \nDoes the loss function matter?", "color":"lightyellow", "key":-43, "group":-54},
{"text":"the covariates is a single Gaussian, \nrather than a Gaussian per class. \n...\nevery example is a support vector,\n...\nmaximum margin algorithm \noutputs the same parameters \nas the OLS algorithm.\n...\naccuracy of the model, \nmeasured using the 0-1 test loss, \nmuch better than its accuracy \nwith respect to the quadratic loss.", "color":"white", "key":-44, "location":"-702.3089459724843 -562.2000597432123", "group":-54},
{"text":"Random \nfeatures\nmodel", "color":"white", "key":-45, "location":"-3105.1629159286276 -1052.1368715803526", "scale":4},
{"text":"Linear \nregression", "color":"white", "key":-46, "location":"-1159.896892956327 913.1143242804461", "scale":4, "group":-53},
{"text":"Empirical", "color":"white", "key":-47, "location":"-3236.1047273924687 1251.9847276883395", "scale":4},
{"text":"Logistic\nregression\n& SVMs", "color":"white", "key":-48, "location":"-165.53961200212552 -618.2924738575488", "scale":4, "group":-54},
{"text":"BLLT", "color":"white", "key":-49, "location":"62.36302278182211 1130.6336607912858", "scale":4, "group":-53},
{"text":"HMRT", "color":"white", "key":-50, "location":"-418.07311857228797 975.4699483845529", "scale":4, "group":-53},
{"text":"MRSY", "color":"white", "key":-51, "location":"-1065.2046578389577 352.62376189764325", "scale":4, "group":-54},
{"text":"MM", "color":"white", "key":-52, "location":"-2953.6867091886893 -152.45473315683387", "scale":4},
{"text":"DKT", "color":"white", "key":-55, "location":"-97.83114837812172 55.1936583731233", "scale":4, "group":-54},
{"text":"BHX", "color":"white", "key":-56, "location":"610.7436304136531 1002.4851871346125", "scale":4, "group":-53},
{"text":"Group", "isGroup":true, "color":"blue", "key":-53},
{"text":"sharp \nasymptotics\n& random \nmatrix \ntheory", "color":"white", "key":-57, "location":"-180.95827130155567 633.8662034715531", "scale":2},
{"text":"precise asymptotics\nfor the generalization error\nof the SVM\nas a function of the\noverparametrization", "color":"white", "key":-58, "location":"-779.9536990569248 1.7088082999877656", "group":-54},
{"location":"173.33167338682188 387.8103883939425", "text":"kammoun2020precise\nKammoun, Abla\nAlouini, Mohamed-Slim\n2020\nOn the precise error analysis \nof support vector machines", "color":"lightyellow", "key":-59, "group":-54},
{"text":"Group", "isGroup":true, "color":"blue", "key":-54},
{"location":"1189.8358112187493 665.1326956293343", "text":"candes2020phase\nCand{\\`e}s, Emmanuel J\nSur, Pragya\nothers\n2020\nThe phase transition for \nthe existence of the MLE \nin high-dimensional \nlogistic regression", "color":"lightyellow", "key":-60},
{"text":"extend their\nresult to the \nnoisy setting", "color":"white", "key":-61, "location":"751.5761150765502 393.07883524418867"}
 ],
  "linkDataArray": [ 
{"from":-6, "to":-3},
{"from":-10, "to":-12},
{"from":-12, "to":-6},
{"from":-11, "to":-15},
{"from":-14, "to":-18},
{"from":-18, "to":-17},
{"from":-6, "to":-21},
{"from":-21, "to":-20},
{"from":-6, "to":-23},
{"from":-23, "to":-22},
{"from":-24, "to":-25},
{"from":-25, "to":-17},
{"from":-19, "to":-26},
{"from":-26, "to":-24},
{"from":-7, "to":-29},
{"from":-29, "to":-27},
{"from":-30, "to":-8},
{"from":-7, "to":-30},
{"from":-17, "to":-32},
{"from":-32, "to":-6},
{"from":-33, "to":-34},
{"from":-33, "to":-36},
{"from":-36, "to":-35},
{"from":-11, "to":-37},
{"from":-37, "to":-24},
{"from":-7, "to":-38},
{"from":-38, "to":-9},
{"from":-39, "to":-10},
{"from":-14, "to":-39},
{"from":-14, "to":-41},
{"from":-41, "to":-16},
{"from":-14, "to":-42},
{"from":-42, "to":-6},
{"from":-14, "to":-44},
{"from":-44, "to":-43},
{"from":-49, "to":-7},
{"from":-50, "to":-8},
{"from":-51, "to":-6},
{"from":-52, "to":-17},
{"from":-55, "to":-10},
{"from":-56, "to":-27},
{"from":-3, "to":-46},
{"from":-10, "to":-57},
{"from":-57, "to":-8},
{"from":-57, "to":-27},
{"from":-43, "to":-58},
{"from":-58, "to":-10},
{"from":-58, "to":-6},
{"from":-61, "to":-60},
{"from":-10, "to":-61},
{"from":-34, "to":-54}
 ]}

---
title: The Weston-Watkins SVM dual problem
author: Yutong Wang
date: '2020-11-06'
slug: a-derivation-of-the-weston-watkins-svm-dual-problem
categories: []
tags: []
bibliography: references.bib  
output:
  blogdown::html_page:
    toc: true
---

In this blog post, we derive the dual problem for the Weston-Watkins support vector machine (SVM).
The dual optimization for the Weston-Watkins SVM can be found in the literature, e.g.,
@keerthi2008sequential
and
@weston1999support.
However, they often omit the tedious derivation.


# Set-up and notations

Let $k \ge 2$ be an integer representing the number of classes and $[k] = \{1,\dots, k\}$.

Let $n$ be the size of the training data.

For each $i \in [n]$, $x_i \in \mathbb{R}^d$ is column vector and $y_i \in [k]$.

Let $W = [w_1,\dots, w_k] \in \mathbb{R}^{d\times k}$ where $w_j$ is the $j$-th column of $W$.

Let $e_i \in \mathbb{R}^k$ be the $i$-th elementary basis (column) vector.

# Primal problem

The Weston-Watkins SVM minimizes over $W$ the following regularized empirical risk:

$$
\frac{1}{2}\|W\|_F^2 + C \sum_{i=1}^n \sum_{j \in [k]: j \ne y_i} \max \{0, 1- (w_{y_i}' x_i - w_j'x_i)\}.
$$

If $\widetilde W = [\widetilde w_1,\dots, \widetilde w_k]$ is the optimizer, then the classifier is
$$
x 
\mapsto 
\mathrm{argmax}_{j \in [k]}
\widetilde w_j 'x.
$$

Now, for each $j, l \in [k]$, let $\Delta_{j,l} = e_j - e_l$.
Then we can rewrite the regularized empirical risk as

$$
\frac{1}{2}\|W\|_F^2 + C \sum_{i=1}^n \sum_{j \in [k]: j \ne y_i} \max \{0, 1- \Delta_{y_i, j}' W' x_i\}
$$



Introducing the slack variable $\xi_{ij}$, we can minimize



$$
\frac{1}{2}\|W\|_F^2 + C \sum_{i=1}^n \sum_{j \in [k]: j \ne y_i} \xi_{ij}
$$

subject to

$$
\begin{cases}
 \xi_{ij} \ge 0 \\
\xi_{ij} \ge 1 - \Delta_{y_i, j}' W' x_i
\end{cases}
$$




or, equivalently, subject to

$$
\begin{cases}
0 \ge 1 - \mathrm{tr}( W' x_i\Delta_{y_i, j}') - \xi_{ij}\\
  0 \ge -\xi_{ij} 
\end{cases}
$$

where $\mathrm{tr}$ is the trace operator. We observe that $W' x_i\Delta_{y_i, j}' \in \mathbb{R}^{k\times k}$.

# Dual problem



Let $\alpha_{ij} \ge 0$ and $\beta_{ij} \ge 0$ be the dual variables for the above constraints, respectively.

The Lagrangian is

$$
L(W, \xi, \alpha, \beta)
=
\frac{1}{2}\|W\|_F^2 
+ \sum_{i=1}^n \sum_{j \in [k]: j \ne y_i} C \xi_{ij}
-
\beta_{ij}\xi_{ij}
+
\alpha_{ij}
(1 - \mathrm{tr}( W' x_i\Delta_{y_i, j}') - \xi_{ij})
$$

Rearranging, we get


$$
L(W, \xi, \alpha, \beta)
=
\frac{1}{2}\|W\|_F^2 
+ \sum_{i=1}^n \sum_{j \in [k]: j \ne y_i} 
   \xi_{ij}
  (C
-
\beta_{ij}
-\alpha_{ij})
+
\alpha_{ij}
(1 - \mathrm{tr}( W' x_i\Delta_{y_i, j}'))
$$


Setting to zero the gradient of $L$ with respect to $W$, we get

$$
0 = \nabla_W L
=
W
- 
  \sum_{i=1}^n \sum_{j \in [k]: j \ne y_i} 
\alpha_{ij}
 x_i\Delta_{y_i, j}'
$$


Equivalently,
$$
W
= 
  \sum_{i=1}^n \sum_{j \in [k]: j \ne y_i} 
\alpha_{ij}
 x_i\Delta_{y_i, j}'
$$


Next, setting to zero the gradient of $L$ with respect to $\xi_{ij}$, we get

$$
0 = \nabla_{\xi_{ij}} L
=
C
-
\beta_{ij} - \alpha_{ij}.
$$

Thus, the dependencies in the Lagrangian on $\xi_{ij}$ and $\beta_{ij}$ are removed and  so


$$
L(W, \alpha)
=
\frac{1}{2}\|W\|_F^2 
+ \sum_{i=1}^n \sum_{j \in [k]: j \ne y_i} 
\alpha_{ij}
(1 - \mathrm{tr}( W' x_i\Delta_{y_i, j}'))
$$

Now,
$$
\sum_{i=1}^n \sum_{j \in [k]: j \ne y_i} 
\alpha_{ij}
\mathrm{tr}( W' x_i\Delta_{y_i, j}')
=
\mathrm{tr}\left(
W'
\sum_{i=1}^n \sum_{j \in [k]: j \ne y_i} 
\alpha_{ij}
 x_i\Delta_{y_i, j}'\right)
=
\mathrm{tr}(W'W)
=
\|W\|_F^2.
$$

Hence, we have


$$
L(W, \alpha)
=
-\frac{1}{2}\|W\|_F^2 
+ \sum_{i=1}^n \sum_{j \in [k]: j \ne y_i} 
\alpha_{ij}
$$


Now, let $\alpha_{iy_i} = -\sum_{j\in[n]: j \ne y_i} \alpha_i$.  Using the definition of $\Delta_{y_i, j} = e_{y_i} - e_j$, we get
$$
W
= 
  \sum_{i=1}^n \sum_{j \in [k]: j \ne y_i} 
\alpha_{ij}
 x_i\Delta_{y_i, j}'
 =
  \sum_{i=1}^n 
  \sum_{j \in [k]: j \ne y_i} 
  -
\alpha_{ij}
 x_i
 e_j'
 +
 \sum_{i=1}^n
 (
  \sum_{j \in [k]: j \ne y_i} 
  \alpha_{ij}
  )
 x_i
 e_{y_i}'
 =
 -
 \sum_{i=1}^n
 \sum_{j \in [k]}
 \alpha_{ij}
 x_i
 e_j'
$$

Now, let us define the column vector
$$
\alpha_i
=
\begin{bmatrix}
\alpha_{i1}\\
\vdots \\
\alpha_{ik}
\end{bmatrix}
\in 
\mathbb{R}^k
$$
Then we have
$$
W = 
-
\sum_{i = 1}^n 
x_i \alpha_i'
$$
where we observe that $x_i \alpha_i' \in \mathbb{R}^{d \times k}$.

Now,
$$
\|W\|_F^2
=
\mathrm{tr}
(W'W)
=
\mathrm{tr}
\left(
\sum_{i, s\in [n]}
\alpha_s x_s' x_i \alpha_i'
\right)
=
\mathrm{tr}
\left(
\sum_{i, s\in [n]}
 x_s' x_i \alpha_i'
\alpha_s
\right)
=
\sum_{i, s\in [n]}
 x_s' x_i \alpha_i'
\alpha_s.
$$
This eliminates the dependencies in the Lagrangian on $W$ and so
$$
L(\alpha)
=
-\frac{1}{2} 
\sum_{i, s\in [n]}
 x_s' x_i \alpha_i'
\alpha_s
+
\sum_{i \in [n]} 
\sum_{j \in [k]: j \ne y_i}
\alpha_{ij}
$$

## Expression for the dual problem

Thus, the dual problem is

$$
\mathrm{minimize}_{\alpha}
\quad
f(\alpha) :=
\frac{1}{2} 
\sum_{i, s\in [n]}
 x_s' x_i \alpha_i'
\alpha_s
-
\sum_{i \in [n]} 
\sum_{j \in [k]: j \ne y_i}
\alpha_{ij}
$$
subject to
$$
\begin{cases}
0 \le \alpha_{ij} \le C &: j \ne y_i\\
\alpha_{iy_i}
=
\sum_{j \in [k]: j \ne y_i} - \alpha_j &: j = y_i.
\end{cases}
$$

This is the same as equation (16) of @keerthi2008sequential. Actually, there is a typo in (16) where the $\alpha_{ij}$s should have a negative sign in front.
Later, @keerthi2008sequential computes the derivative of $f$ with the correct sign.

# Derivatives

Below, fix such a $i \in [n]$ and $j \in [k]$ such that $j \ne y_i$.

In this section, we compute $\frac{\partial f}{\partial \alpha_{ij}}$.

First, let $i \ne s$ and consider the term 
$$
x_s' x_i \alpha_i'\alpha_s
=
x_s' x_i (\alpha_{ij} \alpha_{sj}
+
\alpha_{iy_i} \alpha_{sy_i}
+
\mathrm{constant}
)
$$
where $\mathrm{constant}$ are terms that do not depend on $\alpha_{ij}$.

Taking derivative of $x_s' x_i \alpha_i'\alpha_s$ with respect to $\alpha_{ij}$, we get 

$$
x_s' x_i (\alpha_{sj}
- \alpha_{sy_i})
$$
where we recall that $\alpha_{iy_i} = \sum_{l \in [k]: l \ne y_i} -\alpha_{il}$.


Similarly, when $i = s$, we have that the derivative of $x_i' x_i \alpha_i'\alpha_i$ is

$$
2 x_i' x_i (\alpha_{ij} - \alpha_{i y_i|}).
$$


From this, we get that
$$
\frac{\partial f}{\partial \alpha_{ij}} (\alpha)
=
-1
+
\sum_{s \in [n]} x_i' x_s (\alpha_{sj} - \alpha_{s y_i})
$$

Now,  observe that

$$
\sum_{s \in [n]} x_i' x_s (\alpha_{sj} - \alpha_{s y_i})
=
x_i '
\left(\sum_{s \in [n]} \alpha_{sj} x_s  -\alpha_{s y_i} x_s\right)
$$

Recall from earlier that

$$
W = 
-
\sum_{i = 1}^n 
x_i \alpha_i'
$$

Thus, 
$$
w_j
=
We_j = 
-
\sum_{i = 1}^n 
\alpha_{ij}x_i
\quad \mbox{and} \quad
w_{y_i}=
We_{y_i} = 
-
\sum_{i = 1}^n 
\alpha_{iy_i}x_{i}.
$$


Thus, we get
$$
x_i '
\left(\sum_{s \in [n]} \alpha_{sj} x_s  -\alpha_{s y_i} x_s\right)
=
w_{y_i}' x_i 
- w_{j} ' x_i.
$$


From this, we conclude that
$$
\frac{\partial f}{\partial \alpha_{ij}} (\alpha)
=
w_{y_i}' x_i - w_{j}'x_i
-1.
$$
Since we are minimizing, it is more convenient to consider the negative gradient: 
$$
-\frac{\partial f}{\partial \alpha_{ij}} (\alpha)
=
1-
(w_{y_i}' x_i - w_{j}'x_i).
$$
Note that this equivalent to equation (17) from @keerthi2008sequential.

# Coordinate descent

In this section, we again fix $i \in [n]$ and $j \in [k]$ such that $j \ne y_i$.
We will explain how [Shark](https://github.com/Shark-ML/Shark/blob/7a182c7923e94cf6a8d65b6c92a162bafad8314c/include/shark/Algorithms/QP/QpMcLinear.h#L387) (@igel2008shark) solves the subproblem:

$$
\mathrm{minimize}_{\alpha_{i1},\dots, \alpha_{ik}}
\quad
f(\alpha) :=
\frac{1}{2} 
\sum_{i, s\in [n]}
 x_s' x_i \alpha_i'
\alpha_s
-
\sum_{i \in [n]} 
\sum_{j \in [k]: j \ne y_i}
\alpha_{ij}
$$
subject to
$$
\begin{cases}
0 \le \alpha_{ij} \le C &: j \ne y_i\\
\alpha_{iy_i}
=
\sum_{j \in [k]: j \ne y_i} - \alpha_j &: j = y_i.
\end{cases}
$$

Note that $\alpha_{st}$ is fixed for all $s \ne i$ and all $j \in [k]$. By cycling through the $i\in [n]$ and solving the above subproblem, we get a form of coordinate descent.



We consider the updates 
$$
\begin{cases}
\hat{\alpha}_{st} = \alpha_{st} &: s \ne i \mbox{ or } t \ne j \\
\hat{\alpha}_{ij} = \alpha_{ij} + m & \mbox{otherwise.}
\end{cases}
$$

Similarly, define

$$
\widehat W = [\hat w_1, \cdots \hat w_k]
=
\sum_{i = 1}^n 
x_i \hat \alpha_i'.
$$

Then by construction, we have for $t \in [k]$ that

$$
\hat w_t
=
\begin{cases}
w_t &: t \not \in \{j, y_i\}\\
w_j - mx_i &: t = j \\
w_{y_i} + mx_i &: t = y_i.
\end{cases}
$$

Thus, we have
$$
-\frac{\partial f}{\partial \alpha_{it}}( \hat \alpha)
= 
1 - (\hat w_{y_i} ' x_i - \hat w_t' x_i)
=
\begin{cases}
1 - (w_{y_i} ' x_i - w_j' x_i) - 2 m \|x_i\|^2 &: t = j \\
1 - (w_{y_i} ' x_i - w_t' x_i) - 1 m \|x_i\|^2 &: t \ne j \\
\end{cases}
$$

In other words, we have

$$
-\frac{\partial f}{\partial \alpha_{it}}( \hat \alpha)
=
\begin{cases}
-\frac{\partial f}{\partial \alpha_{ij}}( \alpha) - 2 m \|x_i\|^2 &: t = j \\
-\frac{\partial f}{\partial \alpha_{it}}( \alpha) - 1 m \|x_i\|^2 &: t \ne j. \\
\end{cases}
$$
This is the mathematical justification behind this [code snippet](https://github.com/Shark-ML/Shark/blob/7a182c7923e94cf6a8d65b6c92a162bafad8314c/include/shark/Algorithms/QP/QpMcLinear.h#L480) from Shark.


---
